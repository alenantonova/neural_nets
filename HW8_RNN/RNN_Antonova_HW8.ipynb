{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = 'ololoasdasddqweqw123456789'\n",
    "word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.330726623535156\n",
      "Clip gradient :  1.9298164278136098\n",
      "1.3684765100479126\n",
      "Clip gradient :  1.3428276853224093\n",
      "0.02804422378540039\n",
      "Clip gradient :  0.034508922197576015\n",
      "0.007588386535644531\n",
      "Clip gradient :  0.0098744079562533\n",
      "0.0045318603515625\n",
      "Clip gradient :  0.005874060642420174\n",
      "0.0036134719848632812\n",
      "Clip gradient :  0.0046419927632531995\n",
      "0.0032134056091308594\n",
      "Clip gradient :  0.004109684393575621\n",
      "0.0029816627502441406\n",
      "Clip gradient :  0.0038133222522466516\n",
      "0.0028142929077148438\n",
      "Clip gradient :  0.0036119474516455433\n",
      "0.0026760101318359375\n",
      "Clip gradient :  0.0034555867178719755\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t hello\n",
      "Original:\t hello\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово\n",
    "Сохранить ноутбук с предсказанием и пройденным assert и прислать на почту a.murashev@corp.mail.ru\n",
    "c темой:\n",
    "\n",
    "\n",
    "[МФТИ\\_2019\\_1] ДЗ №8 ФИО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'\n",
    "ds = WordDataSet(word=word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.x2hidden = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "    \n",
    "        self.x2hidden_i = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden_i = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2hidden_f = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden_f = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2hidden_o = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden_o = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.outweight = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "        \n",
    "        self.activation  = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, prev_h_t, prev_c_t):\n",
    "\n",
    "        cand_c_t = self.activation(self.x2hidden(x) + self.hidden(prev_h_t))\n",
    "        \n",
    "        i_t = torch.sigmoid(self.x2hidden_i(x) + self.hidden_i(prev_h_t))\n",
    "        f_t = torch.sigmoid(self.x2hidden_f(x) + self.hidden_f(prev_h_t))\n",
    "        o_t = torch.sigmoid(self.x2hidden_o(x) + self.hidden_o(prev_h_t))\n",
    "  \n",
    "        c_t = f_t * prev_c_t + i_t * cand_c_t\n",
    "        h_t = o_t * self.activation(c_t)\n",
    "        output = self.outweight(h_t)\n",
    "        \n",
    "        return output, h_t, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(in_size=ds.vec_size, hidden_size=50, out_size=ds.vec_size)\n",
    "\n",
    "e_cnt = 150\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lstm_optim     = SGD(lstm.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.96966552734375\n",
      "Clip gradient :  3.9621062044844098\n",
      "55.280662536621094\n",
      "Clip gradient :  5.098674449842935\n",
      "34.50522994995117\n",
      "Clip gradient :  8.13513306276048\n",
      "21.527170181274414\n",
      "Clip gradient :  34.56204257471622\n",
      "26.57499122619629\n",
      "Clip gradient :  58.41753140562637\n",
      "14.22696304321289\n",
      "Clip gradient :  6.567133397173636\n",
      "7.2023749351501465\n",
      "Clip gradient :  7.626819133552264\n",
      "4.366931438446045\n",
      "Clip gradient :  14.046829727545287\n",
      "1.4094419479370117\n",
      "Clip gradient :  2.100908800313955\n",
      "0.2378101348876953\n",
      "Clip gradient :  0.3912073353628567\n",
      "0.05113792419433594\n",
      "Clip gradient :  0.06200934561740256\n",
      "0.027177810668945312\n",
      "Clip gradient :  0.03964232164435101\n",
      "0.0171966552734375\n",
      "Clip gradient :  0.018677042502100977\n",
      "0.013371467590332031\n",
      "Clip gradient :  0.012534196348760218\n",
      "0.01151275634765625\n",
      "Clip gradient :  0.010331602728238633\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.ones(lstm.hidden.in_features)\n",
    "    ct = torch.zeros(lstm.x2hidden.out_features)\n",
    "    loss = 0\n",
    "    lstm_optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh, ct = lstm(x, hh, ct)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=1)\n",
    "    \n",
    "    lstm_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "lstm.eval()\n",
    "hh = torch.ones(lstm.hidden.in_features)\n",
    "ct = torch.zeros(lstm.x2hidden.out_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh, ct = lstm(x, hh, ct)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super(GRU, self).__init__()\n",
    "    \n",
    "        self.x2hidden = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2hidden_u = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden_u = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2hidden_r = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden_r = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.outweight = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "        \n",
    "        self.activation  = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, prev_h_t):\n",
    "        u_t = torch.sigmoid(self.x2hidden_u(x) + self.hidden_u(prev_h_t))\n",
    "        r_t = torch.sigmoid(self.x2hidden_r(x) + self.hidden_r(prev_h_t))\n",
    "  \n",
    "        cand_h_t = self.activation(self.x2hidden(x) + self.hidden(r_t * prev_h_t))\n",
    "        h_t = (1 - u_t) * cand_h_t + u_t * prev_h_t\n",
    "        output = self.outweight(h_t)\n",
    "        \n",
    "        return output, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = GRU(in_size=ds.vec_size, hidden_size=10, out_size=ds.vec_size)\n",
    "\n",
    "e_cnt = 200\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "gru_optim     = SGD(gru.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.05367279052734\n",
      "Clip gradient :  6.235991867136826\n",
      "56.00344467163086\n",
      "Clip gradient :  6.957778339586844\n",
      "42.672813415527344\n",
      "Clip gradient :  9.708163068013185\n",
      "28.70158576965332\n",
      "Clip gradient :  9.12639295982216\n",
      "16.049579620361328\n",
      "Clip gradient :  6.809344451313966\n",
      "6.815958023071289\n",
      "Clip gradient :  3.791361181334377\n",
      "2.3238205909729004\n",
      "Clip gradient :  1.175163635020065\n",
      "1.58056640625\n",
      "Clip gradient :  0.502730965172851\n",
      "0.5121030807495117\n",
      "Clip gradient :  0.5655030457760515\n",
      "0.1083526611328125\n",
      "Clip gradient :  0.34056821766071343\n",
      "0.0673370361328125\n",
      "Clip gradient :  0.13985752193442502\n",
      "0.04840660095214844\n",
      "Clip gradient :  0.0508732845766967\n",
      "0.039559364318847656\n",
      "Clip gradient :  0.04899905854695647\n",
      "0.03411674499511719\n",
      "Clip gradient :  0.02998454006374314\n",
      "0.030307769775390625\n",
      "Clip gradient :  0.020850171696827752\n",
      "0.0274200439453125\n",
      "Clip gradient :  0.015985771883065446\n",
      "0.025112152099609375\n",
      "Clip gradient :  0.013821690372286696\n",
      "0.023184776306152344\n",
      "Clip gradient :  0.012914704887505497\n",
      "0.021528244018554688\n",
      "Clip gradient :  0.012074222081638558\n",
      "0.020078659057617188\n",
      "Clip gradient :  0.01129375345489847\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.ones(gru.hidden.in_features)\n",
    "    loss = 0\n",
    "    gru_optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh= gru(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(gru.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(gru.parameters(), max_norm=1)\n",
    "    \n",
    "    gru_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "gru.eval()\n",
    "hh = torch.ones(gru.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = gru(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
